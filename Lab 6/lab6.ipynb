{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe63371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\facultation\\ML\\ML-Lab1\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import List\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_DIR=\"models\"\n",
    "# Can change back to \"mistralai/Mistral-7B-v0.3\" afterwards\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL_NAME = \"google/flan-t5-large\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "TOP_K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26bb7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(path: str) -> str:\n",
    "    logger.info(f\"Loading PDF: {path}\")\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text: str, chunk_sz: int, ol: int) -> List[str]:\n",
    "    logger.info(\"Chunking text with RecursiveCharacterTextSplitter\")\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_sz,\n",
    "        chunk_overlap=ol,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\n\\n\",\n",
    "            \". \",\n",
    "            \"\\n\",\n",
    "            \" \",\n",
    "            \"\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_text(text)\n",
    "    logger.info(f\"Created {len(chunks)} chunks\")\n",
    "    chunks = [\n",
    "        c.strip()\n",
    "        for c in chunks\n",
    "        if len(c.strip()) > 150\n",
    "        and not c.strip().startswith((\">>>\", \"```\"))\n",
    "        and not c.strip().lower().startswith((\"chapter\", \"table of contents\"))\n",
    "    ]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f49a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniRAG:\n",
    "    def __init__(self):\n",
    "        logger.info(\"Initializing model\")\n",
    "\n",
    "        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "        self.llm = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL_NAME)\n",
    "\n",
    "\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "\n",
    "    def build_index(self, documents: List[str]):\n",
    "        logger.info(\"Creating embeddings\")\n",
    "        embeddings = self.embedding_model.encode(documents, show_progress_bar=True)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "        dim = embeddings.shape[1]\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        self.index.add(embeddings)\n",
    "        self.chunks = documents\n",
    "\n",
    "        logger.info(\"FAISS index created\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = TOP_K) -> List[str]:\n",
    "        logger.info(\"Retrieving relevant chunks\")\n",
    "        query_embedding = self.embedding_model.encode([query]).astype(\"float32\")\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        _, indices = self.index.search(query_embedding, k)\n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            print(f\"Score {i}: {scores[0][i]:.3f}\")\n",
    "        return [self.chunks[i] for i in indices[0]]\n",
    "\n",
    "    def generate(self, query: str, context_chunks: List[str]) -> str:\n",
    "        logger.info(\"Generating answer\")\n",
    "        context = \"\\n\\n\".join(context_chunks)\n",
    "        prompt = (\n",
    "            \"You are a technical assistant. \"\n",
    "            \"Answer the question clearly and concisely using ONLY the information in the context. \"\n",
    "            \"You may summarize or combine information across chunks. \"\n",
    "            \"Ignore code snippets unless they directly answer the question. \"\n",
    "            \"If the context is completely irrelevant, respond exactly: 'Not found in context.'\\n\\n\"\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Question:\\n{query}\\n\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        output_ids = self.llm.generate(**inputs, max_new_tokens=200)\n",
    "        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    def ask(self, query: str) -> str:\n",
    "        retrieved_chunks = self.retrieve(query)\n",
    "        for i, c in enumerate(retrieved_chunks):\n",
    "            print(f\"\\n Retrieved chunk {i+1} \")\n",
    "            print(c[:300])\n",
    "\n",
    "        return self.generate(query, retrieved_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b5d8126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing model\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:__main__:Loading PDF: s1.pdf\n",
      "INFO:__main__:Chunking text with RecursiveCharacterTextSplitter\n",
      "INFO:__main__:Created 4123 chunks\n",
      "INFO:__main__:Creating embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046dc6443a794b58ad5875c06aee550f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:FAISS index created\n",
      "INFO:__main__:Retrieving relevant chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7568a0fa15fd4f3da5906b036f527060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating answer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.621\n",
      "Score 1: 0.606\n",
      "Score 2: 0.606\n",
      "Score 3: 0.604\n",
      "\n",
      " Retrieved chunk 1 \n",
      ". If a model suffers from overfitting, we also \n",
      "say that the model has a high variance, which can be caused by having too many parameters, leading \n",
      "to a model that is too complex given the underlying data\n",
      "\n",
      " Retrieved chunk 2 \n",
      "is one approach to tackling the problem of overfitting by adding additional information and thereby \n",
      "shrinking the parameter values of the model to induce a penalty against complexity\n",
      "\n",
      " Retrieved chunk 3 \n",
      ". To address this problem of overfitting, \n",
      "we can collect more training data, reduce the complexity of the model, or increase the regularization \n",
      "parameter, for example.\n",
      "For unregularized models, it can also help to decrease the number of features via feature selection \n",
      "(Chapter 4) or feature extrac\n",
      "\n",
      " Retrieved chunk 4 \n",
      ". The reason for the overfitting is that our model is too complex for \n",
      "the given training data. Common solutions to reduce the generalization error are as follows:\n",
      "• Collect more training data\n",
      "• Introduce a penalty for complexity via regularization\n",
      "• Choose a simpler model with fewer parameters\n",
      "• Re\n",
      "Not found in context\n"
     ]
    }
   ],
   "source": [
    "rag = MiniRAG()\n",
    "\n",
    "text = load_pdf(\"s1.pdf\")\n",
    "chunks = chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "rag.build_index(chunks)\n",
    "\n",
    "question = \"When does overfitting happen?\"\n",
    "answer = rag.ask(question)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3203e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PDF_FILES = [\"s1.pdf\", \"s2.pdf\"]\n",
    "OUTPUT_MODEL_PATH = \"finetuned_model_local\"\n",
    "\n",
    "BASE_EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL_NAME = \"google/flan-t5-large\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "TOP_K = 4\n",
    "\n",
    "def load_pdfs(file_paths: List[str]) -> str:\n",
    "    combined_text = \"\"\n",
    "    for path in file_paths:\n",
    "        if os.path.exists(path):\n",
    "            logger.info(f\"Loading PDF: {path}\")\n",
    "            reader = PdfReader(path)\n",
    "            for page in reader.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    combined_text += text + \"\\n\"\n",
    "        else:\n",
    "            logger.warning(f\"File not found: {path}\")\n",
    "    return combined_text\n",
    "\n",
    "def chunk_text(text: str, chunk_sz: int, ol: int) -> List[str]:\n",
    "    logger.info(\"Chunking text with RecursiveCharacterTextSplitter\")\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_sz,\n",
    "        chunk_overlap=ol,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\n\\n\",\n",
    "            \". \",\n",
    "            \"\\n\",\n",
    "            \" \",\n",
    "            \"\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_text(text)\n",
    "    logger.info(f\"Created {len(chunks)} chunks\")\n",
    "    chunks = [\n",
    "        c.strip()\n",
    "        for c in chunks\n",
    "        if len(c.strip()) > 150\n",
    "        and not c.strip().startswith((\">>>\", \"```\"))\n",
    "        and not c.strip().lower().startswith((\"chapter\", \"table of contents\"))\n",
    "    ]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b94dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fine_tuning(train_chunks: List[str], base_model_name: str, output_path: str):\n",
    "    logger.info(f\"Starting fine-tuning on {len(train_chunks)} chunks...\")\n",
    "    \n",
    "    model = SentenceTransformer(base_model_name)\n",
    "    \n",
    "    train_examples = []\n",
    "    for chunk in train_chunks:\n",
    "        train_examples.append(\n",
    "            InputExample(texts=[chunk, chunk])\n",
    "        )\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "    \n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "    \n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=1,\n",
    "        show_progress_bar=True,\n",
    "        output_path=output_path\n",
    "    )\n",
    "    logger.info(f\"Fine-tuning complete. Model saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8026ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTunedRAG:\n",
    "    def __init__(self, embedding_model_path: str):\n",
    "        logger.info(\"Initializing RAG with Fine-Tuned Model\")\n",
    "\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_path)\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "        self.llm = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL_NAME)\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "\n",
    "    def build_index(self, documents: List[str]):\n",
    "        logger.info(\"Creating embeddings for index\")\n",
    "        embeddings = self.embedding_model.encode(documents, show_progress_bar=True)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "        dim = embeddings.shape[1]\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        self.index.add(embeddings)\n",
    "        self.chunks = documents\n",
    "        logger.info(\"FAISS index built.\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = TOP_K) -> List[str]:\n",
    "        query_embedding = self.embedding_model.encode([query]).astype(\"float32\")\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        _, indices = self.index.search(query_embedding, k)\n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            print(f\"Score {i}: {scores[0][i]:.3f}\")\n",
    "        return [self.chunks[i] for i in indices[0]]\n",
    "\n",
    "    def generate(self, query: str, context_chunks: List[str]) -> str:\n",
    "        context = \"\\n\\n\".join(context_chunks)\n",
    "        prompt = (\n",
    "            \"You are a technical assistant. \"\n",
    "            \"Answer the question clearly and concisely using ONLY the information in the context. \"\n",
    "            \"You may summarize or combine information across chunks. \"\n",
    "            \"Ignore code snippets unless they directly answer the question. \"\n",
    "            \"If the context is completely irrelevant, respond exactly: 'Not found in context.'\\n\\n\"\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Question:\\n{query}\\n\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        output_ids = self.llm.generate(**inputs, max_new_tokens=200)\n",
    "        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    def ask(self, query: str) -> str:\n",
    "        logger.info(f\"Querying: {query}\")\n",
    "        retrieved_chunks = self.retrieve(query)\n",
    "        for i, c in enumerate(retrieved_chunks):\n",
    "            print(f\"\\nRetrieved chunk {i+1}\")\n",
    "            print(c[:300])\n",
    "\n",
    "        return self.generate(query, retrieved_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05497400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading PDF: s1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading PDF: s2.pdf\n",
      "INFO:__main__:Chunking text with RecursiveCharacterTextSplitter\n",
      "INFO:__main__:Created 8171 chunks\n",
      "INFO:__main__:Starting fine-tuning on 7562 chunks...\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 1: Fine-Tuning Embeddings ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099004918ad348269a4e6b8f2154c494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\facultation\\ML\\ML-Lab1\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='473' max='473' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [473/473 50:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Save model to finetuned_model_local\n",
      "INFO:__main__:Fine-tuning complete. Model saved to: finetuned_model_local\n",
      "INFO:__main__:Initializing RAG with Fine-Tuned Model\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: finetuned_model_local\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 2: Running RAG ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating embeddings for index\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975632bb301d41f88b8ce825c42ec9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:FAISS index built.\n",
      "INFO:__main__:Querying: What is underfitting?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5484e36812443ea7f154d45d18e3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.517\n",
      "Score 1: 0.507\n",
      "Score 2: 0.503\n",
      "Score 3: 0.498\n",
      "\n",
      "Retrieved chunk 1\n",
      ". We can also see that the \n",
      "training accuracy increases for training datasets with fewer than 250 examples, and the gap between \n",
      "validation and training accuracy widens—an indicator of an increasing degree of overfitting.\n",
      "Addressing over- and underfitting with validation curves\n",
      "Validation curves are\n",
      "\n",
      "Retrieved chunk 2\n",
      ". As we discussed in Chapter 3, A Tour of Machine Learning \n",
      "Classifiers Using Scikit-Learn, overfitting means the model fits the parameters too closely with regard \n",
      "to the particular observations in the training dataset but does not generalize well to new data; we say \n",
      "that the model has a high vari\n",
      "\n",
      "Retrieved chunk 3\n",
      ". The reason for the overfitting is that our model is too complex for \n",
      "the given training data. Common solutions to reduce the generalization error are as follows:\n",
      "• Collect more training data\n",
      "• Introduce a penalty for complexity via regularization\n",
      "• Choose a simpler model with fewer parameters\n",
      "• Re\n",
      "\n",
      "Retrieved chunk 4\n",
      ". Overfitting, which we will return \n",
      "to later in this chapter, means that the model captures the patterns in the training data \n",
      "well but fails to generalize well to unseen data.\n",
      "A Tour of Machine Learning Classifiers Using Scikit-Learn 58\n",
      "    # plot the decision surface\n",
      "    x1_min, x1_max = X[:, 0].\n",
      "What is underfitting?\n",
      "Not found in context.\n"
     ]
    }
   ],
   "source": [
    "raw_text = load_pdfs(PDF_FILES)\n",
    "\n",
    "doc_chunks = chunk_text(raw_text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "print(\"\\n--- Phase 1: Fine-Tuning Embeddings ---\\n\")\n",
    "run_fine_tuning(doc_chunks, BASE_EMBEDDING_MODEL, OUTPUT_MODEL_PATH)\n",
    "\n",
    "print(\"\\n--- Phase 2: Running RAG ---\\n\")\n",
    "rag = FineTunedRAG(embedding_model_path=OUTPUT_MODEL_PATH)\n",
    "rag.build_index(doc_chunks)\n",
    "\n",
    "question = \"What is underfitting?\" \n",
    "    \n",
    "answer = rag.ask(question)\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cca24cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Querying: Explain the concept of overfitting.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8373a2c5e22441b9c8ef6077e03019b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.606\n",
      "Score 1: 0.601\n",
      "Score 2: 0.579\n",
      "Score 3: 0.573\n",
      "\n",
      "Retrieved chunk 1\n",
      ". If a model suffers from overfitting, we also \n",
      "say that the model has a high variance, which can be caused by having too many parameters, leading \n",
      "to a model that is too complex given the underlying data\n",
      "\n",
      "Retrieved chunk 2\n",
      ". As we discussed in Chapter 3, A Tour of Machine Learning \n",
      "Classifiers Using Scikit-Learn, overfitting means the model fits the parameters too closely with regard \n",
      "to the particular observations in the training dataset but does not generalize well to new data; we say \n",
      "that the model has a high vari\n",
      "\n",
      "Retrieved chunk 3\n",
      ". To address this problem of overfitting, \n",
      "we can collect more training data, reduce the complexity of the model, or increase the regularization \n",
      "parameter, for example.\n",
      "For unregularized models, it can also help to decrease the number of features via feature selection \n",
      "(Chapter 4) or feature extrac\n",
      "\n",
      "Retrieved chunk 4\n",
      "is one approach to tackling the problem of overfitting by adding additional information and thereby \n",
      "shrinking the parameter values of the model to induce a penalty against complexity\n",
      "Explain the concept of overfitting.\n",
      "Overfitting means the model fits the parameters too closely with regard to the particular observations in the training dataset but does not generalize well to new data; we say that the model has a high variance.\n"
     ]
    }
   ],
   "source": [
    "question = \"Explain the concept of overfitting.\"\n",
    "answer = rag.ask(question)\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54db9a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Querying: How can overfitting be prevented?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2782fe41c1db47b0a21dc7fd022b1688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.701\n",
      "Score 1: 0.643\n",
      "Score 2: 0.597\n",
      "Score 3: 0.593\n",
      "\n",
      "Retrieved chunk 1\n",
      ". To address this problem of overfitting, \n",
      "we can collect more training data, reduce the complexity of the model, or increase the regularization \n",
      "parameter, for example.\n",
      "For unregularized models, it can also help to decrease the number of features via feature selection \n",
      "(Chapter 4) or feature extrac\n",
      "\n",
      "Retrieved chunk 2\n",
      ". The reason for the overfitting is that our model is too complex for \n",
      "the given training data. Common solutions to reduce the generalization error are as follows:\n",
      "• Collect more training data\n",
      "• Introduce a penalty for complexity via regularization\n",
      "• Choose a simpler model with fewer parameters\n",
      "• Re\n",
      "\n",
      "Retrieved chunk 3\n",
      ". \n",
      "Then, to prevent overfitting, we can apply one or multiple regularization schemes to achieve good \n",
      "generalization performance on new data, such as the held-out test dataset.\n",
      "In Chapters 3 and 4, we covered L1 and L2 regularization. Both techniques can prevent or reduce the \n",
      "effect of overfitting \n",
      "\n",
      "Retrieved chunk 4\n",
      "is one approach to tackling the problem of overfitting by adding additional information and thereby \n",
      "shrinking the parameter values of the model to induce a penalty against complexity\n",
      "How can overfitting be prevented?\n",
      "To address this problem of overfitting, we can collect more training data, reduce the complexity of the model, or increase the regularization parameter, for example.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can overfitting be prevented?\"\n",
    "answer = rag.ask(question)\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25a75294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Querying: What is the ensemble method?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0f451b90804a98ac69b1c850ef1fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.726\n",
      "Score 1: 0.657\n",
      "Score 2: 0.623\n",
      "Score 3: 0.618\n",
      "\n",
      "Retrieved chunk 1\n",
      ". This section will introduce a basic explanation \n",
      "of how ensembles work and why they are typically recognized for yielding a good generalization \n",
      "performance.\n",
      "In this chapter, we will focus on the most popular ensemble methods that use the majority voting \n",
      "principle. Majority voting simply means th\n",
      "\n",
      "Retrieved chunk 2\n",
      ". Depending on the \n",
      "technique, the ensemble can be built from different classification algorithms, for example, decision \n",
      "trees, support vector machines, logistic regression classifiers, and so on. Alternatively, we can also use \n",
      "the same base classification algorithm, fitting different subsets of t\n",
      "\n",
      "Retrieved chunk 3\n",
      ". For example, assuming that we col-\n",
      "lected predictions from 10 experts, ensemble methods would allow us to strategically combine those \n",
      "predictions by the 10 experts to come up with a prediction that was more accurate and robust than the \n",
      "predictions by each individual expert. As you will see later\n",
      "\n",
      "Retrieved chunk 4\n",
      ". Figure 7.2  \n",
      "illustrates the concept of a general ensemble approach using majority voting:\n",
      "Figure 7.2: A general ensemble approach\n",
      "To predict a class label via simple majority or plurality voting, we can combine the predicted class \n",
      "labels of each individual classifier, C j, and select the class l\n",
      "What is the ensemble method?\n",
      "Majority voting.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the ensemble method?\"\n",
    "answer = rag.ask(question)\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81b0ba",
   "metadata": {},
   "source": [
    "EVALUATION SCALE:\n",
    "\n",
    "1 = Incorrect / hallucinated;\n",
    "2 = Partially correct but incomplete or weakly grounded;\n",
    "3 = Mostly correct and grounded, minor omissions;\n",
    "4 = Fully correct, complete, and grounded in context;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e985c880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4bb75e6c9f54ee5a0ceda395bf665eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.628\n",
      "Score 1: 0.609\n",
      "Score 2: 0.600\n",
      "Score 3: 0.593\n",
      "Question: Explain the concept of overfitting\n",
      "Answer: The reason for the overfitting is that our model is too complex for the given training data.\n",
      "LLM-as-a-Judge Score: 4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def llm_as_judge(question: str, answer: str, context: str, tokenizer, llm) -> int:\n",
    "    \"\"\"\n",
    "    Uses the LLM to evaluate the answer on a scale of 1-4.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Evaluate the following answer on a scale of 1-4 based on the provided context:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Evaluation Scale:\n",
    "1 = Incorrect / hallucinated;\n",
    "2 = Partially correct but incomplete or weakly grounded;\n",
    "3 = Mostly correct and grounded, minor omissions;\n",
    "4 = Fully correct, complete, and grounded in context;\n",
    "\n",
    "Provide only the number (1, 2, 3, or 4) as your response.\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    output_ids = llm.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    match = re.search(r'\\b([1-4])\\b', response)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        try:\n",
    "            return int(response.split()[0]) if response.split()[0].isdigit() else 1\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "question = \"Explain the concept of overfitting\"\n",
    "retrieved_chunks = rag.retrieve(question)\n",
    "context = \"\\n\\n\".join(retrieved_chunks)\n",
    "answer = rag.generate(question, retrieved_chunks)\n",
    "\n",
    "score = llm_as_judge(question, answer, context, rag.tokenizer, rag.llm)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"LLM-as-a-Judge Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1282478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b7c2c4d2924fa4a3bc521cb045322a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.701\n",
      "Score 1: 0.643\n",
      "Score 2: 0.597\n",
      "Score 3: 0.593\n",
      "Question: How can overfitting be prevented?\n",
      "Answer: To address this problem of overfitting, we can collect more training data, reduce the complexity of the model, or increase the regularization parameter, for example.\n",
      "LLM-as-a-Judge Score: 4\n"
     ]
    }
   ],
   "source": [
    "question = \"How can overfitting be prevented?\"\n",
    "\n",
    "retrieved_chunks = rag.retrieve(question)\n",
    "context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "answer = rag.generate(question, retrieved_chunks)\n",
    "\n",
    "score = llm_as_judge(question, answer, context, rag.tokenizer, rag.llm)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"LLM-as-a-Judge Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_explicit(question: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the question contains explicit or inappropriate content.\n",
    "    \"\"\"\n",
    "    explicit_keywords = [\"kill\", \"murder\", \"bomb\", \"drug\", \"fuck\", \"shit\", \"sex\", \"porn\", \"nude\", \"ass\", \"bitch\", \"bastard\"]\n",
    "    return any(keyword in question.lower() for keyword in explicit_keywords)\n",
    "\n",
    "def guarded_ask(rag, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Guarded version of ask that checks for explicit content and off-topic questions.\n",
    "    \"\"\"\n",
    "    if is_explicit(question):\n",
    "        return \"Warning: Inappropriate question detected.\"\n",
    "            \n",
    "    query_embedding = rag.embedding_model.encode([question]).astype(\"float32\")\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    scores, indices = rag.index.search(query_embedding, TOP_K)\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        print(f\"Score {i}: {scores[0][i]:.3f}\")\n",
    "    \n",
    "    if max(scores[0]) < 0.5:\n",
    "        return \"Warning: This question appears to be off-topic. I can only answer questions related to machine learning concepts from the provided documents.\"\n",
    "    else:\n",
    "        answer = rag.ask(question)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c00f9257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757b31e774a9478f84aca50c9b855efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Received query: how can overfitting be prevented?\n",
      "INFO:root:Query passed safety guard\n",
      "INFO:root:Retrieving relevant chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.714\n",
      "Score 1: 0.660\n",
      "Score 2: 0.614\n",
      "Score 3: 0.606\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0835d77cb24202935d8cb2ff8b8e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Retrieved chunk 1 with score 0.714\n",
      "INFO:root:Retrieved chunk 2 with score 0.660\n",
      "INFO:root:Retrieved chunk 3 with score 0.614\n",
      "INFO:root:Chunk 1 preview:\n",
      ". To address this problem of overfitting, \n",
      "we can collect more training data, reduce the complexity of the model, or increase the regularization \n",
      "parameter, for example.\n",
      "For unregularized models, it c\n",
      "INFO:root:Chunk 2 preview:\n",
      ". The reason for the overfitting is that our model is too complex for \n",
      "the given training data. Common solutions to reduce the generalization error are as follows:\n",
      "• Collect more training data\n",
      "• Intro\n",
      "INFO:root:Chunk 3 preview:\n",
      "is one approach to tackling the problem of overfitting by adding additional information and thereby \n",
      "shrinking the parameter values of the model to induce a penalty against complexity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how can overfitting be prevented?\n",
      "Answer: Answer: How can overfitting be prevented? Answer:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdebcfa6bec4cebae814fa5267d66cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.213\n",
      "Score 1: 0.209\n",
      "Score 2: 0.203\n",
      "Score 3: 0.191\n",
      "Question: who won the 2018 world cup?\n",
      "Answer: Warning: This question appears to be off-topic. I can only answer questions related to machine learning concepts from the provided documents.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c246d23213e48d0a391388912615ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.262\n",
      "Score 1: 0.244\n",
      "Score 2: 0.243\n",
      "Score 3: 0.239\n",
      "Question: how to make a bomb\n",
      "Answer: Warning: This question appears to be off-topic. I can only answer questions related to machine learning concepts from the provided documents.\n"
     ]
    }
   ],
   "source": [
    "# Test the guardrail with three questions\n",
    "question1 = \"how can overfitting be prevented?\"\n",
    "answer1 = guarded_ask(rag, question1)\n",
    "print(f\"Question: {question1}\")\n",
    "print(f\"Answer: {answer1}\")\n",
    "print()\n",
    "\n",
    "question2 = \"who won the 2018 world cup?\"\n",
    "answer2 = guarded_ask(rag, question2)\n",
    "print(f\"Question: {question2}\")\n",
    "print(f\"Answer: {answer2}\")\n",
    "print()\n",
    "\n",
    "question3 = \"how to make a bomb\"\n",
    "answer3 = guarded_ask(rag, question3)\n",
    "print(f\"Question: {question3}\")\n",
    "print(f\"Answer: {answer3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec33940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7bac89abb249b78f774982000dceb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Received query: How can overfitting be prevented?\n",
      "INFO:root:Query passed safety guard\n",
      "INFO:root:Retrieving relevant chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.714\n",
      "Score 1: 0.660\n",
      "Score 2: 0.614\n",
      "Score 3: 0.606\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0358a3a3b3af4aa29b56f5fc8368e9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Retrieved chunk 1 with score 0.714\n",
      "INFO:root:Retrieved chunk 2 with score 0.660\n",
      "INFO:root:Retrieved chunk 3 with score 0.614\n",
      "INFO:root:Chunk 1 preview:\n",
      ". To address this problem of overfitting, \n",
      "we can collect more training data, reduce the complexity of the model, or increase the regularization \n",
      "parameter, for example.\n",
      "For unregularized models, it c\n",
      "INFO:root:Chunk 2 preview:\n",
      ". The reason for the overfitting is that our model is too complex for \n",
      "the given training data. Common solutions to reduce the generalization error are as follows:\n",
      "• Collect more training data\n",
      "• Intro\n",
      "INFO:root:Chunk 3 preview:\n",
      "is one approach to tackling the problem of overfitting by adding additional information and thereby \n",
      "shrinking the parameter values of the model to induce a penalty against complexity\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m output_text = scrolledtext.ScrolledText(root, width=\u001b[32m80\u001b[39m, height=\u001b[32m20\u001b[39m)\n\u001b[32m     22\u001b[39m output_text.pack()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mroot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\tkinter\\__init__.py:1599\u001b[39m, in \u001b[36mMisc.mainloop\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1597\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmainloop\u001b[39m(\u001b[38;5;28mself\u001b[39m, n=\u001b[32m0\u001b[39m):\n\u001b[32m   1598\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1599\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "\n",
    "def on_ask():\n",
    "    question = entry.get()\n",
    "    answer = guarded_ask(rag, question)\n",
    "    if not answer.startswith(\"Warning\"):\n",
    "        idx = answer.find(\"Answer:\")\n",
    "        if idx != -1:\n",
    "            answer = answer[idx + 7:].strip()\n",
    "    output_text.delete(1.0, tk.END)\n",
    "    output_text.insert(tk.END, f\"Question: {question}\\n\\nAnswer: {answer}\")\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"RAG Question GUI\")\n",
    "\n",
    "tk.Label(root, text=\"Enter your question:\").pack()\n",
    "entry = tk.Entry(root, width=50)\n",
    "entry.pack()\n",
    "tk.Button(root, text=\"Ask\", command=on_ask).pack()\n",
    "output_text = scrolledtext.ScrolledText(root, width=80, height=20)\n",
    "output_text.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5971c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
