{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "abe63371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_DIR=\"models\"\n",
    "# Can change back to \"mistralai/Mistral-7B-v0.3\" afterwards\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL_NAME = \"google/flan-t5-large\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "TOP_K = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c26bb7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(path: str) -> str:\n",
    "    logger.info(f\"Loading PDF: {path}\")\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text: str, chunk_sz: int, ol: int) -> List[str]:\n",
    "    logger.info(\"Chunking text with RecursiveCharacterTextSplitter\")\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_sz,\n",
    "        chunk_overlap=ol,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\n\\n\",\n",
    "            \". \",\n",
    "            \"\\n\",\n",
    "            \" \",\n",
    "            \"\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_text(text)\n",
    "    logger.info(f\"Created {len(chunks)} chunks\")\n",
    "    chunks = [\n",
    "        c.strip()\n",
    "        for c in chunks\n",
    "        if len(c.strip()) > 150\n",
    "        and not c.strip().startswith((\">>>\", \"```\"))\n",
    "        and not c.strip().lower().startswith((\"chapter\", \"table of contents\"))\n",
    "    ]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f49a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniRAG:\n",
    "    def __init__(self):\n",
    "        logger.info(\"Initializing model\")\n",
    "\n",
    "        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "        self.llm = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL_NAME)\n",
    "\n",
    "\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "\n",
    "    def build_index(self, documents: List[str]):\n",
    "        logger.info(\"Creating embeddings\")\n",
    "        embeddings = self.embedding_model.encode(documents, show_progress_bar=True)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "        dim = embeddings.shape[1]\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        self.index.add(embeddings)\n",
    "        self.chunks = documents\n",
    "\n",
    "        logger.info(\"FAISS index created\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = TOP_K) -> List[str]:\n",
    "        logger.info(\"Retrieving relevant chunks\")\n",
    "        query_embedding = self.embedding_model.encode([query]).astype(\"float32\")\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        _, indices = self.index.search(query_embedding, k)\n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            print(f\"Score {i}: {scores[0][i]:.3f}\")\n",
    "        return [self.chunks[i] for i in indices[0]]\n",
    "\n",
    "    def generate(self, query: str, context_chunks: List[str]) -> str:\n",
    "        logger.info(\"Generating answer\")\n",
    "        context = \"\\n\\n\".join(context_chunks)\n",
    "        prompt = (\n",
    "            \"You are a technical assistant. \"\n",
    "            \"Answer the question clearly and concisely using ONLY the information in the context. \"\n",
    "            \"You may summarize or combine information across chunks. \"\n",
    "            \"Ignore code snippets unless they directly answer the question. \"\n",
    "            \"If the context is completely irrelevant, respond exactly: 'Not found in context.'\\n\\n\"\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Question:\\n{query}\\n\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        output_ids = self.llm.generate(**inputs, max_new_tokens=200)\n",
    "        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    def ask(self, query: str) -> str:\n",
    "        retrieved_chunks = self.retrieve(query)\n",
    "        for i, c in enumerate(retrieved_chunks):\n",
    "            print(f\"\\n Retrieved chunk {i+1} \")\n",
    "            print(c[:300])\n",
    "\n",
    "        return self.generate(query, retrieved_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d8126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing Pytorch-based models\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:__main__:Loading PDF: s1.pdf\n",
      "INFO:__main__:Chunking text with RecursiveCharacterTextSplitter\n",
      "INFO:__main__:Created 4123 chunks\n",
      "INFO:__main__:Creating embeddings\n",
      "Batches: 100%|██████████| 117/117 [01:18<00:00,  1.48it/s]\n",
      "INFO:__main__:FAISS index created\n",
      "INFO:__main__:Retrieving relevant chunks\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.90it/s]\n",
      "INFO:__main__:Generating answer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.621\n",
      "Score 1: 0.606\n",
      "Score 2: 0.606\n",
      "Score 3: 0.604\n",
      "\n",
      "---- Retrieved chunk 1 ----\n",
      ". If a model suffers from overfitting, we also \n",
      "say that the model has a high variance, which can be caused by having too many parameters, leading \n",
      "to a model that is too complex given the underlying data\n",
      "\n",
      "---- Retrieved chunk 2 ----\n",
      "is one approach to tackling the problem of overfitting by adding additional information and thereby \n",
      "shrinking the parameter values of the model to induce a penalty against complexity\n",
      "\n",
      "---- Retrieved chunk 3 ----\n",
      ". To address this problem of overfitting, \n",
      "we can collect more training data, reduce the complexity of the model, or increase the regularization \n",
      "parameter, for example.\n",
      "For unregularized models, it can also help to decrease the number of features via feature selection \n",
      "(Chapter 4) or feature extrac\n",
      "\n",
      "---- Retrieved chunk 4 ----\n",
      ". The reason for the overfitting is that our model is too complex for \n",
      "the given training data. Common solutions to reduce the generalization error are as follows:\n",
      "• Collect more training data\n",
      "• Introduce a penalty for complexity via regularization\n",
      "• Choose a simpler model with fewer parameters\n",
      "• Re\n",
      "\n",
      "--- Answer ---\n",
      "\n",
      "Not found in context\n"
     ]
    }
   ],
   "source": [
    "rag = MiniRAG()\n",
    "\n",
    "text = load_pdf(\"s1.pdf\")\n",
    "chunks = chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "rag.build_index(chunks)\n",
    "\n",
    "question = \"When does overfitting happen?\"\n",
    "answer = rag.ask(question)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3203e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PDF_FILES = [\"s1.pdf\", \"s2.pdf\"]\n",
    "OUTPUT_MODEL_PATH = \"finetuned_model_local\"\n",
    "\n",
    "BASE_EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL_NAME = \"google/flan-t5-large\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "TOP_K = 4\n",
    "\n",
    "def load_pdfs(file_paths: List[str]) -> str:\n",
    "    combined_text = \"\"\n",
    "    for path in file_paths:\n",
    "        if os.path.exists(path):\n",
    "            logger.info(f\"Loading PDF: {path}\")\n",
    "            reader = PdfReader(path)\n",
    "            for page in reader.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    combined_text += text + \"\\n\"\n",
    "        else:\n",
    "            logger.warning(f\"File not found: {path}\")\n",
    "    return combined_text\n",
    "\n",
    "def chunk_text(text: str, chunk_sz: int, ol: int) -> List[str]:\n",
    "    logger.info(\"Chunking text with RecursiveCharacterTextSplitter\")\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_sz,\n",
    "        chunk_overlap=ol,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\n\\n\",\n",
    "            \". \",\n",
    "            \"\\n\",\n",
    "            \" \",\n",
    "            \"\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_text(text)\n",
    "    logger.info(f\"Created {len(chunks)} chunks\")\n",
    "    chunks = [\n",
    "        c.strip()\n",
    "        for c in chunks\n",
    "        if len(c.strip()) > 150\n",
    "        and not c.strip().startswith((\">>>\", \"```\"))\n",
    "        and not c.strip().lower().startswith((\"chapter\", \"table of contents\"))\n",
    "    ]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b94dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fine_tuning(train_chunks: List[str], base_model_name: str, output_path: str):\n",
    "    logger.info(f\"Starting fine-tuning on {len(train_chunks)} chunks...\")\n",
    "    \n",
    "    model = SentenceTransformer(base_model_name)\n",
    "    \n",
    "    train_examples = []\n",
    "    for chunk in train_chunks:\n",
    "        train_examples.append(\n",
    "            InputExample(texts=[chunk, chunk])\n",
    "        )\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "    \n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "    \n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=1,\n",
    "        show_progress_bar=True,\n",
    "        output_path=output_path\n",
    "    )\n",
    "    logger.info(f\"Fine-tuning complete. Model saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FineTunedRAG:\n",
    "    def __init__(self, embedding_model_path: str):\n",
    "        logger.info(\"Initializing RAG with Fine-Tuned Model\")\n",
    "\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_path)\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
    "        self.llm = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL_NAME)\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "\n",
    "    def build_index(self, documents: List[str]):\n",
    "        logger.info(\"Creating embeddings for index\")\n",
    "        embeddings = self.embedding_model.encode(documents, show_progress_bar=True)\n",
    "        embeddings = np.array(embeddings).astype(\"float32\")\n",
    "        dim = embeddings.shape[1]\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        self.index.add(embeddings)\n",
    "        self.chunks = documents\n",
    "        logger.info(\"FAISS index built.\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = TOP_K) -> List[str]:\n",
    "        query_embedding = self.embedding_model.encode([query]).astype(\"float32\")\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        _, indices = self.index.search(query_embedding, k)\n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            print(f\"Score {i}: {scores[0][i]:.3f}\")\n",
    "        return [self.chunks[i] for i in indices[0]]\n",
    "\n",
    "    def generate(self, query: str, context_chunks: List[str]) -> str:\n",
    "        context = \"\\n\\n\".join(context_chunks)\n",
    "        prompt = (\n",
    "            \"You are a technical assistant. \"\n",
    "            \"Answer the question clearly and concisely using ONLY the information in the context. \"\n",
    "            \"You may summarize or combine information across chunks. \"\n",
    "            \"Ignore code snippets unless they directly answer the question. \"\n",
    "            \"If the context is completely irrelevant, respond exactly: 'Not found in context.'\\n\\n\"\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Question:\\n{query}\\n\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        output_ids = self.llm.generate(**inputs, max_new_tokens=200)\n",
    "        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    def ask(self, query: str) -> str:\n",
    "        logger.info(f\"Querying: {query}\")\n",
    "        retrieved_chunks = self.retrieve(query)\n",
    "        for i, c in enumerate(retrieved_chunks):\n",
    "            print(f\"\\nRetrieved chunk {i+1}\")\n",
    "            print(c[:300])\n",
    "\n",
    "        return self.generate(query, retrieved_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "05497400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading PDF: s1.pdf\n",
      "INFO:__main__:Loading PDF: s2.pdf\n",
      "INFO:__main__:Chunking text with RecursiveCharacterTextSplitter\n",
      "INFO:__main__:Created 8171 chunks\n",
      "INFO:__main__:Starting fine-tuning on 7562 chunks...\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 1: Fine-Tuning Embeddings ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='473' max='473' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [473/473 41:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Save model to finetuned_model_local\n",
      "INFO:__main__:Fine-tuning complete. Model saved to: finetuned_model_local\n",
      "INFO:__main__:Initializing RAG with Fine-Tuned Model...\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: finetuned_model_local\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 2: Running RAG ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating embeddings for index...\n",
      "Batches: 100%|██████████| 237/237 [02:52<00:00,  1.38it/s]\n",
      "INFO:__main__:FAISS index built.\n",
      "INFO:__main__:Querying: What is underfitting?\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.518\n",
      "Score 1: 0.508\n",
      "Score 2: 0.505\n",
      "Score 3: 0.498\n",
      "\n",
      "---- Retrieved chunk 1 ----\n",
      ". We can also see that the \n",
      "training accuracy increases for training datasets with fewer than 250 examples, and the gap between \n",
      "validation and training accuracy widens—an indicator of an increasing degree of overfitting.\n",
      "Addressing over- and underfitting with validation curves\n",
      "Validation curves are\n",
      "\n",
      "---- Retrieved chunk 2 ----\n",
      ". As we discussed in Chapter 3, A Tour of Machine Learning \n",
      "Classifiers Using Scikit-Learn, overfitting means the model fits the parameters too closely with regard \n",
      "to the particular observations in the training dataset but does not generalize well to new data; we say \n",
      "that the model has a high vari\n",
      "\n",
      "---- Retrieved chunk 3 ----\n",
      ". The reason for the overfitting is that our model is too complex for \n",
      "the given training data. Common solutions to reduce the generalization error are as follows:\n",
      "• Collect more training data\n",
      "• Introduce a penalty for complexity via regularization\n",
      "• Choose a simpler model with fewer parameters\n",
      "• Re\n",
      "\n",
      "---- Retrieved chunk 4 ----\n",
      ". Overfitting, which we will return \n",
      "to later in this chapter, means that the model captures the patterns in the training data \n",
      "well but fails to generalize well to unseen data.\n",
      "A Tour of Machine Learning Classifiers Using Scikit-Learn 58\n",
      "    # plot the decision surface\n",
      "    x1_min, x1_max = X[:, 0].\n",
      "What is underfitting?\n",
      "Not found in context.\n"
     ]
    }
   ],
   "source": [
    "raw_text = load_pdfs(PDF_FILES)\n",
    "\n",
    "doc_chunks = chunk_text(raw_text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "\n",
    "print(\"\\n--- Phase 1: Fine-Tuning Embeddings ---\\n\")\n",
    "run_fine_tuning(doc_chunks, BASE_EMBEDDING_MODEL, OUTPUT_MODEL_PATH)\n",
    "\n",
    "print(\"\\n--- Phase 2: Running RAG ---\\n\")\n",
    "rag = FineTunedRAG(embedding_model_path=OUTPUT_MODEL_PATH)\n",
    "rag.build_index(doc_chunks)\n",
    "\n",
    "question = \"What is underfitting?\" \n",
    "    \n",
    "answer = rag.ask(question)\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1cca24cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Querying: Explain the concept of overfitting.\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 76.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.606\n",
      "Score 1: 0.602\n",
      "Score 2: 0.582\n",
      "Score 3: 0.576\n",
      "\n",
      "---- Retrieved chunk 1 ----\n",
      ". If a model suffers from overfitting, we also \n",
      "say that the model has a high variance, which can be caused by having too many parameters, leading \n",
      "to a model that is too complex given the underlying data\n",
      "\n",
      "---- Retrieved chunk 2 ----\n",
      ". As we discussed in Chapter 3, A Tour of Machine Learning \n",
      "Classifiers Using Scikit-Learn, overfitting means the model fits the parameters too closely with regard \n",
      "to the particular observations in the training dataset but does not generalize well to new data; we say \n",
      "that the model has a high vari\n",
      "\n",
      "---- Retrieved chunk 3 ----\n",
      ". To address this problem of overfitting, \n",
      "we can collect more training data, reduce the complexity of the model, or increase the regularization \n",
      "parameter, for example.\n",
      "For unregularized models, it can also help to decrease the number of features via feature selection \n",
      "(Chapter 4) or feature extrac\n",
      "\n",
      "---- Retrieved chunk 4 ----\n",
      "is one approach to tackling the problem of overfitting by adding additional information and thereby \n",
      "shrinking the parameter values of the model to induce a penalty against complexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the concept of overfitting.\n",
      "Overfitting means the model fits the parameters too closely with regard to the particular observations in the training dataset but does not generalize well to new data; we say that the model has a high variance.\n"
     ]
    }
   ],
   "source": [
    "question = \"Explain the concept of overfitting.\"\n",
    "answer = rag.ask(question)\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db9a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Querying: How can overfitting be prevented?\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.701\n",
      "Score 1: 0.646\n",
      "Score 2: 0.595\n",
      "Score 3: 0.593\n",
      "\n",
      "---- Retrieved chunk 1 ----\n",
      ". To address this problem of overfitting, \n",
      "we can collect more training data, reduce the complexity of the model, or increase the regularization \n",
      "parameter, for example.\n",
      "For unregularized models, it can also help to decrease the number of features via feature selection \n",
      "(Chapter 4) or feature extrac\n",
      "\n",
      "---- Retrieved chunk 2 ----\n",
      ". The reason for the overfitting is that our model is too complex for \n",
      "the given training data. Common solutions to reduce the generalization error are as follows:\n",
      "• Collect more training data\n",
      "• Introduce a penalty for complexity via regularization\n",
      "• Choose a simpler model with fewer parameters\n",
      "• Re\n",
      "\n",
      "---- Retrieved chunk 3 ----\n",
      "is one approach to tackling the problem of overfitting by adding additional information and thereby \n",
      "shrinking the parameter values of the model to induce a penalty against complexity\n",
      "\n",
      "---- Retrieved chunk 4 ----\n",
      ". \n",
      "Then, to prevent overfitting, we can apply one or multiple regularization schemes to achieve good \n",
      "generalization performance on new data, such as the held-out test dataset.\n",
      "In Chapters 3 and 4, we covered L1 and L2 regularization. Both techniques can prevent or reduce the \n",
      "effect of overfitting \n",
      "How can overfitting be prevented?\n",
      "To address this problem of overfitting, we can collect more training data, reduce the complexity of the model, or increase the regularization parameter, for example.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can overfitting be prevented?\"\n",
    "answer = rag.ask(question)\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "25a75294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Querying: What is the ensemble method?\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.729\n",
      "Score 1: 0.661\n",
      "Score 2: 0.626\n",
      "Score 3: 0.622\n",
      "\n",
      "---- Retrieved chunk 1 ----\n",
      ". This section will introduce a basic explanation \n",
      "of how ensembles work and why they are typically recognized for yielding a good generalization \n",
      "performance.\n",
      "In this chapter, we will focus on the most popular ensemble methods that use the majority voting \n",
      "principle. Majority voting simply means th\n",
      "\n",
      "---- Retrieved chunk 2 ----\n",
      ". Depending on the \n",
      "technique, the ensemble can be built from different classification algorithms, for example, decision \n",
      "trees, support vector machines, logistic regression classifiers, and so on. Alternatively, we can also use \n",
      "the same base classification algorithm, fitting different subsets of t\n",
      "\n",
      "---- Retrieved chunk 3 ----\n",
      ". For example, assuming that we col-\n",
      "lected predictions from 10 experts, ensemble methods would allow us to strategically combine those \n",
      "predictions by the 10 experts to come up with a prediction that was more accurate and robust than the \n",
      "predictions by each individual expert. As you will see later\n",
      "\n",
      "---- Retrieved chunk 4 ----\n",
      ". Figure 7.2  \n",
      "illustrates the concept of a general ensemble approach using majority voting:\n",
      "Figure 7.2: A general ensemble approach\n",
      "To predict a class label via simple majority or plurality voting, we can combine the predicted class \n",
      "labels of each individual classifier, C j, and select the class l\n",
      "What is the ensemble method?\n",
      "Majority voting.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the ensemble method?\"\n",
    "answer = rag.ask(question)\n",
    "print(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e656b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab1-YjpxjkQf-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
